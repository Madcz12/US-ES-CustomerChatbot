# -*- coding: utf-8 -*-
"""ChatbotV2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tzk-PZCxqvH7KjTGtg9SqCMH79ZVnDA-
"""

import random
import json
import pickle
import numpy as np
import nltk
from nltk import WordNetLemmatizer
from keras.optimizers import SGD
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.optimizers import legacy as keras_legacy
from keras.optimizers import SGD

from tensorflow.python.keras.optimizers import *

lemmatizer = WordNetLemmatizer()

with open('intentos.json', 'r', encoding='utf-8') as f:
    intents = json.load(f)

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

words = []
classes = []
documents = []
ignore_letters = ['?', '!', '¿', '.', ',']

for intent in intents['intentos']:
    for language in ['patterns', 'patterns_es']:
        for pattern in intent.get(language, []):
            word_list = nltk.word_tokenize(pattern)
            words.extend(word_list)
            documents.append((word_list, intent.get('tag', intent.get('tag_es', ''))))
            if intent.get('tag', intent.get('tag_es', '')) not in classes:
                classes.append(intent.get('tag', intent.get('tag_es', '')))

words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]
words = sorted(set(words))

pickle.dump(words, open('words.pkl', 'wb'))
pickle.dump(classes, open('classes.pkl', 'wb'))

#Pasa la información a unos y ceros según las palabras presentes en cada categoría para hacer el entrenamiento
training = np.zeros((len(documents), len(words)))
output = np.zeros((len(documents), len(classes)))
for i, document in enumerate(documents):
    word_patterns = document[0]
    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]
    for word in word_patterns:
        if word in words:
            training[i, words.index(word)] = 1
    output[i, classes.index(document[1])] = 1

# Mezcla los datos de entrenamiento
data = list(zip(training, output))
random.shuffle(data)
training, output = zip(*data)

training = np.array(training)
output = np.array(output)

train_x = training
train_y = output

#Creamos la red neuronal
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

#Creamos el optimizador y lo compilamos
sgd = SGD(learning_rate=0.001, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])

#Entrenamos el modelo y lo guardamos
train_process = model.fit(train_x, train_y, epochs=100, batch_size=5, verbose=1)
model.save("chatbot_model.h5", train_process)

import nltk
from nltk.stem import WordNetLemmatizer

from keras.models import load_model

# Importamos los archivos generados en el código anterior
with open('intentos.json', 'r', encoding='utf-8') as f:
    intents = json.load(f)
    print(intents)  # Agregar esta línea
words = pickle.load(open('words.pkl', 'rb'))
classes = pickle.load(open('classes.pkl', 'rb'))
model = load_model('chatbot_model.h5')

# Pasamos las palabras de oración a su forma raíz
def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]
    return sentence_words

# Convertimos la información a unos y ceros según si están presentes en los patrones
def bag_of_words(sentence, language):
    sentence_words = clean_up_sentence(sentence)
    bag = [0]*len(words)
    for w in sentence_words:
        for i, word in enumerate(words):
            if word == w:
                bag[i]=1
    return np.array(bag)

# Predecimos la categoría a la que pertenece la oración
def predict_class(sentence, language):
    bow = bag_of_words(sentence, language)
    res = model.predict(np.array([bow]))[0]
    max_index = np.where(res ==np.max(res))[0][0]
    category = classes[max_index]
    return category

# Obtenemos una respuesta aleatoria
def get_response(tag, language, intents_json):
    list_of_intents = intents_json['intentos']
    result = ""
    for i in list_of_intents:
        if language == 'en':
            if i.get('tag') == tag:
                result = random.choice(i['responses'])
                break
        elif language == 'es':
            if i.get('tag_es') == tag:
                result = random.choice(i['responses_es'])
                break
    return result

# Ejecutamos el chat en bucle
while True:
    message = input("")
    language = 'en' if all(ord(c) < 128 for c in message) else 'es'
    ints = predict_class(message, language)
    res = get_response(ints, language, intents)
    print(res)